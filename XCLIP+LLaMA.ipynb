{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4432,"status":"ok","timestamp":1727919763338,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"},"user_tz":-330},"id":"7zvIF8gO0cj-","outputId":"a7ba4977-b6d0-42e6-8c52-d2e8a702df41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["**Import Necessary Libraries**"],"metadata":{"id":"CbeeAqNrMzS9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpB4EZH3SowQ"},"outputs":[],"source":["%%capture\n","!pip install peft\n","%%capture\n","!pip install bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXqmA627ZTwU"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import bitsandbytes as bnb\n","from tqdm import tqdm\n","from torch.utils.data import Dataset , DataLoader\n","from transformers import AutoProcessor , XCLIPVisionModel , LlamaForCausalLM , LlamaTokenizer , AutoTokenizer , DataCollatorWithPadding\n","from peft import get_peft_model, LoraConfig, TaskType\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6KK6sn0br7o"},"outputs":[],"source":["x_clip_model_path = \"microsoft/xclip-base-patch16\"\n","llama_model_path = 'meta-llama/Llama-3.2-3B-Instruct'\n","lora_rank = 16\n","lora_alpha = 16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhk-4hUEzV9Q"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv('/content/test_df_with_link.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tF_91ZQgS22M"},"outputs":[],"source":["import asyncio\n","import aiohttp\n","import os\n","import pathlib\n","from urllib.parse import urlparse\n","\n","GIF_PATH = pathlib.Path('gifs')\n","if not GIF_PATH.exists():\n","    GIF_PATH.mkdir()\n","\n","def is_valid_url(url):\n","    \"\"\"Check if the URL is a valid string and properly formatted.\"\"\"\n","    if not isinstance(url, str):\n","        return False\n","    parsed = urlparse(url)\n","    return all([parsed.scheme, parsed.netloc])\n","\n","async def download_file(url):\n","    filename = url.split(\"/\")[-1]\n","    filepath = GIF_PATH / filename\n","\n","    # Check if the GIF file already exists\n","    if filepath.exists():\n","#         print(f\"File {filename} already exists, skipping download.\")\n","        return\n","\n","    max_retries = 3  # Set the maximum number of retries\n","    retries = 0\n","    while retries < max_retries:\n","        try:\n","            async with aiohttp.ClientSession(trust_env=True) as session:\n","                async with session.get(url) as response:\n","                    if response.status == 200:\n","                        with open(filepath, mode=\"wb\") as file:\n","                            while True:\n","                                chunk = await response.content.read(1024)\n","                                if not chunk:\n","                                    break\n","                                file.write(chunk)\n","#                         print(f\"Downloaded file {filename}\")\n","                        return  # Exit the loop if successful\n","                    else:\n","#                         print(f\"Failed to download {url}: HTTP Status {response.status}\")\n","                        return\n","        except (aiohttp.client_exceptions.ClientConnectorError, ConnectionResetError) as e:\n","#             print(f\"Error downloading {url}: {e}\")\n","            retries += 1\n","            await asyncio.sleep(1)  # Wait for a second before retrying\n","\n","async def safe_request(semaphore, url):\n","    async with semaphore:\n","        return await download_file(url)\n","\n","async def main(url_col, parallel_processes):\n","    # Filter out invalid URLs\n","    valid_urls = [url for url in url_col if is_valid_url(url)]\n","\n","    semaphore = asyncio.Semaphore(parallel_processes)\n","    tasks = [asyncio.ensure_future(safe_request(semaphore, url)) for url in valid_urls]\n","    await asyncio.gather(*tasks)"]},{"cell_type":"markdown","source":["**Downloading the Data**"],"metadata":{"id":"9UDyYXeQNVxw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mceel1b2S6OU"},"outputs":[],"source":["await main(df['full_link'][:20000], 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbAWaSQOWV1W"},"outputs":[],"source":["df_new = df[:20000]"]},{"cell_type":"markdown","source":["**DEFINE THE PRETRAINED TEXT TOKENIZER : SINCE THIS TOKENIZER DID NOT HAVE PRE-BUILT PAD TOKEN WE INCLUDED OUR CUSTOM PAD TOKEN**"],"metadata":{"id":"25rW-oZZNHY-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":710,"status":"ok","timestamp":1727919801160,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"},"user_tz":-330},"id":"kCXojBM3S7ll","outputId":"fc6bd386-c8b2-461b-f50a-a7642eb74b4d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(llama_model_path , use_auth_token = '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')"]},{"cell_type":"markdown","source":["**SETTING THE PAD TOKEN**"],"metadata":{"id":"Ir2AT3pKSlcS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Xgp4pzQTD_Z"},"outputs":[],"source":["tokenizer.add_special_tokens({'pad_token': '<|pad_token|>'})\n","tokenizer.pad_token_id = 128010"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1727919804064,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"},"user_tz":-330},"id":"7Ticq0K6TFbl","outputId":"cb3d8ece-7b94-435f-aa8b-422a457b0365"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<|python_tag|>'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.pad_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQKxktTCkSeH"},"outputs":[],"source":["data_collator_for_padding = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4422,"status":"ok","timestamp":1727917040665,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"},"user_tz":-330},"id":"zEFH4vQpB-Sv","outputId":"b263029d-3daf-4bb3-a4fc-e7760fbefe5e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["# # XCLIPProcesser\n","image_processor = AutoProcessor.from_pretrained(x_clip_model_path)"]},{"cell_type":"markdown","source":["**MAKE THE DATASET**"],"metadata":{"id":"QglEfHx1OOfE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFz1bSF8bR0I"},"outputs":[],"source":["from PIL import Image\n","import numpy as np\n","class VQA_DATASET(Dataset):\n","    def __init__(self,df,gif_dir,tokenizer,testing = False , num_frames = 16):\n","        super().__init__()\n","        self.df = df\n","        self.gif_dir = gif_dir\n","        self.num_frames = num_frames\n","        self.tokenizer = tokenizer\n","        self.testing = testing\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def sample_gif_frames(self, gif_path, num_frames=32):\n","\n","        # Open the GIF file using PIL\n","        gif = Image.open(gif_path)\n","\n","        # Extract all frames from the GIF\n","        frames = []\n","        try:\n","            while True:\n","                frame = gif.copy().convert(\"RGB\")  # Convert each frame to RGB\n","                frames.append(np.array(frame))  # Convert to numpy array\n","                gif.seek(gif.tell() + 1)  # Move to the next frame\n","        except EOFError:\n","            pass  # End of GIF reached\n","\n","        total_frames = len(frames)\n","\n","        # If there are fewer than num_frames, pad with the last frame\n","        if total_frames < num_frames:\n","            last_frame = frames[-1]  # Get the last frame\n","            for _ in range(num_frames - total_frames):\n","                frames.append(last_frame)\n","\n","        # If there are more than num_frames, sample the frames evenly\n","        indices = np.linspace(0, len(frames) - 1, num=num_frames, dtype=int)\n","        sampled_frames = [frames[i] for i in indices]\n","\n","        # Convert the list of frames into a single NumPy array\n","        result = np.stack(sampled_frames)\n","\n","        return result\n","\n","    def __getitem__(self,idx):\n","      try:\n","          # Attempt to construct the gif_path\n","          gif_path = self.df['full_link'][idx].split('/')[-1]\n","          gif_path = os.path.join(self.gif_dir, gif_path)\n","\n","          # Check if the gif_path exists\n","          if os.path.exists(gif_path):\n","              frames = self.sample_gif_frames(gif_path, self.num_frames)\n","              inputs = inputs.pixel_values\n","              question = self.df['question'][idx]\n","              answer = self.df['answer'][idx]\n","\n","              if self.testing:\n","                  input_ids, mask = self.create_sequence(question)\n","                  sample = {\n","                      'gif_embed': inputs,\n","                      'input_ids': input_ids,\n","                      'mask': mask,\n","                      'question': question,\n","                      'answer': answer\n","                  }\n","                  return sample\n","              else:\n","                  input_ids, mask = self.create_sequence(question, answer)\n","                  sample = sample = {\n","                      'gif_embed': inputs,\n","                      'input_ids': input_ids,\n","                      'mask': mask\n","                  }\n","                  return sample\n","          else:\n","              # If the file does not exist, return None\n","              return None\n","      except Exception as e:\n","        # Handle any exception and return None\n","        return None\n","\n","    def create_sequence(self,question = None ,answer=None ):\n","      if self.testing == True:\n","        prompt = f\"<|begin_of_text|> question : {question} answer : \"\n","        sequence = self.tokenizer(str(prompt)  ,max_length = 32 ,padding = 'max_length' , truncation = True ,return_tensors='pt' )\n","        input_ids = sequence['input_ids']\n","        mask = sequence['attention_mask']\n","        return input_ids , mask\n","\n","      else:\n","        prompt = f\"<|begin_of_text|> question : {question} answer : {answer} <|eot_id|>\"\n","        sequence = self.tokenizer(str(prompt)  ,max_length = 32 ,padding = 'max_length' , truncation = True ,return_tensors='pt' )\n","        input_ids = sequence['input_ids']\n","        mask = sequence['attention_mask']\n","        return input_ids , mask\n","\n","from torch.utils.data import default_collate\n","\n","def custom_collator(batch):\n","    # Filter out None items from the batch\n","    batch = [item for item in batch if item is not None]\n","\n","    if len(batch) == 0:\n","        return None  # If all items are None, return None or handle it accordingly\n","\n","    gif_embed = [item['gif_embed'] for item in batch]\n","    input_ids = [item['input_ids'] for item in batch]\n","    masks = [item['mask'] for item in batch]\n","\n","    # Prepare features for padding\n","    features_to_pad = {\n","        'input_ids': input_ids,\n","        'attention_mask': masks,\n","    }\n","\n","    # Use the collator to pad input_ids and masks\n","    padded_tensors = data_collator_for_padding(features_to_pad)\n","\n","    # Use default_collate for gif_embed to stack tensors along the batch dimension\n","    gif_embed_collated = default_collate(gif_embed)\n","\n","    return padded_tensors, gif_embed_collated\n","\n","\n","def test_collator(batch):\n","    gif_embed = [item['gif_embed'] for item in batch]\n","    input_ids = [item['input_ids'] for item in batch]\n","    masks = [item['mask'] for item in batch]\n","    questions = [item['question'] for item in batch]\n","    answers = [item['answer'] for item in batch]\n","\n","    features_to_pad = {\n","        'input_ids': input_ids,\n","        'attention_mask': masks,\n","    }\n","\n","    padded_tensors = data_collator_for_padding(features_to_pad)\n","\n","    return padded_tensors , gif_embed , questions , answers"]},{"cell_type":"markdown","source":["**MAKE THE MODEL**"],"metadata":{"id":"ZQS5DmlzOVys"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7KAd7QLT3RE"},"outputs":[],"source":["class VQAModel(nn.Module):\n","    def __init__(self  , x_clip_model_path , llama_model_path, r ,lora_alpha ,lora_dropout = 0.1 ,tgt = [\"q_proj\",'k_proj']):\n","        super(VQAModel , self).__init__()\n","        self.video_encoder = XCLIPVisionModel.from_pretrained(x_clip_model_path)\n","        for params in self.video_encoder.parameters():\n","            params.requires_grad = False\n","\n","        self.llama_model  = LlamaForCausalLM.from_pretrained(llama_model_path , use_auth_token = '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'   , device_map = 'auto', load_in_4bit=True , bnb_4bit_compute_dtype=torch.bfloat16 , bnb_4bit_quant_type=\"nf4\" )\n","        self.peft_config = LoraConfig(\n","          task_type=TaskType.CAUSAL_LM, inference_mode=False,\n","          r=r,\n","          lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","          target_modules = tgt\n","        )\n","        self.peft_model = get_peft_model(self.llama_model, self.peft_config).to(device)\n","\n","        self.MLP = nn.Sequential( nn.Linear(768, 2048),\n","                                  nn.Dropout(p=0.1),\n","                                  nn.GELU(),\n","                                  nn.Linear(2048, 3072),\n","                              ).to(device)\n","\n","    def forward(self , input_ids ,\n","                attention_mask = None,\n","                video_pixel_values = None\n","                ):\n","\n","        encoded_videos = self.video_encoder(video_pixel_values) # B*Num_Frames , CLS , 768\n","        encoded_videos = encoded_videos.pooler_output # B*Num_Frames , 768\n","        encoded_videos = encoded_videos.view(-1,16,768).to(device) # B , 16 , 768\n","\n","        embeddings = self.peft_model.base_model.model.model.embed_tokens(input_ids)\n","        encoded_videos = self.MLP(encoded_videos) # B x 16 x 3072\n","        # b x seq\n","        new_mask = (torch.ones((attention_mask.shape[0] , 16))).to(device)\n","        attention_mask = torch.cat([\n","            new_mask , attention_mask\n","        ] , dim = 1)\n","\n","        fused_embeddings = torch.cat([\n","           encoded_videos , embeddings] , dim = 1) # b x 48 x 3072\n","\n","        outputs = self.peft_model(inputs_embeds = fused_embeddings.half() , attention_mask = attention_mask.half() )\n","        final_output = {\n","            \"logits\" : outputs.logits\n","            }\n","\n","        return final_output\n","\n","    def generate(self,gif_embeds,input_ids,mask):   ##for testing and generation\n","        with torch.no_grad():\n","          encoded_videos = (self.video_encoder(gif_embeds)).to(device)\n","          encoded_videos = encoded_videos.pooler_output\n","          encoded_videos = encoded_videos.view(-1,16,768)\n","          embeddings = self.peft_model.base_model.model.model.embed_tokens(input_ids)\n","          encoded_videos = self.MLP(encoded_videos)\n","          fused_embeddings = torch.cat([\n","              encoded_videos , embeddings.squeeze(dim=1)] , dim = 1)  # b x 48 x 3072\n","          mask = mask.to(device)\n","          new_mask = (torch.ones((mask.shape[0] , 16))).to(device)\n","          mask = torch.cat([\n","              new_mask , mask.squeeze(dim=1)\n","          ] , dim = 1).to(device)\n","\n","        return fused_embeddings , mask\n"]},{"cell_type":"markdown","source":["**INSTANTIATE THE MODEL**"],"metadata":{"id":"a6EBnz2MOZ-T"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["496e851106d5423588a946c22327fd83","98c092af2c6d43eea4c14fd2b250fb7c","e64464dde4ec494e9db3d9a72010cf46","740f4fb9448a4c2a9c22fd8c81442a5b","7a787f25ff424ac5b33fade4baf2184d","a0f692048fd24ec093967104cc119030","c7441276316e48b2bdcef12cfc0e06c1","506ede5c124b46a39fa79031c52dd630","a4c4e9d45c9b49938ac2f3625b3a72a5","e420c295c2f04e34bcb790489aad1b59","f73f9ea555194ab58f4046f766c33c74"]},"executionInfo":{"elapsed":9641,"status":"ok","timestamp":1727919885102,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"},"user_tz":-330},"id":"KwTVmVIyU521","outputId":"5295d539-a6b0-4665-fd9b-57ebde16e629"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3220: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n","The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"496e851106d5423588a946c22327fd83","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model = VQAModel(x_clip_model_path, llama_model_path, lora_rank, lora_alpha).to(device)"]},{"cell_type":"code","source":["import torch\n","\n","checkpoint_path = 'your_checkpoint_path'\n","\n","checkpoint = torch.load(checkpoint_path)\n","\n","model_state_dict = checkpoint.get('model_state_dict', checkpoint)\n","model.load_state_dict(model_state_dict, strict=False)\n","\n","\n","model.to(device)\n"],"metadata":{"id":"py7STNm-WB0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1727919893304,"user":{"displayName":"Jishu Sengupta","userId":"06508002032974920597"},"user_tz":-330},"id":"Rm26eA4bVFrf","outputId":"0ca73312-b9f2-4ed7-e5f2-82cb4d2dba0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Parameters: 1937174016\n","Trainable Parameters: 12456960\n"]}],"source":["# Assuming 'model' is your PyTorch model\n","def count_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","    return total_params, trainable_params\n","\n","total_params, trainable_params = count_parameters(model)\n","\n","print(f'Total Parameters: {total_params}')\n","print(f'Trainable Parameters: {trainable_params}')"]},{"cell_type":"markdown","source":["**INSTANTIATE A TRAIN DATASET AND A TRAIN DATALOADER**"],"metadata":{"id":"LZVgfgX5OePs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JareUwHYnFCO"},"outputs":[],"source":["train_dataset = VQA_DATASET(df_new , gif_dir = '/content/gifs' , tokenizer = tokenizer)\n","train_dataloader = DataLoader(train_dataset , batch_size = 8 , shuffle = True , collate_fn = custom_collator)"]},{"cell_type":"markdown","source":["**DEFINE THE OPTIMIZERS AND LR_SCHEDULER**"],"metadata":{"id":"8y1pT51xOn_U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzOsKrQOVVVf"},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n","from transformers import get_scheduler\n","## lr_scheduler\n","num_train_epochs = 10\n","num_update_steps_per_epoch = len(train_dataloader)\n","# print(num_update_steps_per_epoch)\n","num_training_steps = num_train_epochs * num_update_steps_per_epoch\n","# print(num_training_steps)\n","\n","lr_scheduler = get_scheduler(\n","    name=\"linear\",  ##Cosine Annealing or any other\n","    optimizer=optimizer,\n","    num_warmup_steps=0.1*num_training_steps,\n","    num_training_steps=num_training_steps,\n",")"]},{"cell_type":"markdown","source":["**MAKE A TRAINING LOOP AND SAVE THE MODEL**"],"metadata":{"id":"MOXRnzGeO1oC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNuv6eEdejd1"},"outputs":[],"source":["import torch.nn.functional as F\n","def train(model,optimizer,device,epochs):\n","        for epoch in range(epochs):\n","            model.train()\n","            loss = 0\n","            with tqdm(total=len(train_dataloader), desc='Training Epoch {}'.format(epoch + 1)) as pbar:\n","              for batch in train_dataloader:\n","                  data,gif_embeds = batch\n","                  gif_embeds = gif_embeds.squeeze(dim = 1).to(device)\n","                  batch_size, frames, channels ,height, width = gif_embeds.shape\n","                  gif_embeds = gif_embeds.view(batch_size * frames, channels, height, width)\n","                  data = {k:v.to(device) for k,v in data.items()}\n","                  optimizer.zero_grad()\n","                  final_output = model(input_ids = data['input_ids'].squeeze(dim=1), attention_mask = data['attention_mask'].squeeze(dim=1) , video_pixel_values = gif_embeds )\n","                  logits = final_output['logits'].to(device)\n","                  trimmed_logits = logits[:, 16:47, :].contiguous()\n","                  trimmed_labels = data['input_ids'].squeeze(dim=1)[:, 1:].contiguous()\n","                  loss = F.cross_entropy(trimmed_logits.view(-1, trimmed_logits.size(-1)), trimmed_labels.view(-1))\n","                  loss.backward()\n","                  optimizer.step()\n","                  lr_scheduler.step()\n","                  loss+= loss.item()\n","                  pbar.update(1)\n","\n","            avg_loss = loss/len(train_dataloader)\n","            print(f'epoch no: {epoch + 1} ||Train_loss : {avg_loss}')   #batch avg loss in every epoch\n","\n","            torch.save({\n","                      'model_state_dict': model.state_dict()\n","                  }, '/content/drive/MyDrive/DataDownload/checkpoint_x_clip_llama.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"FDnY2C2koTVc","outputId":"073596fb-dcc6-4702-9466-ccddea2d7f0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1: 100%|██████████| 2500/2500 [46:01<00:00,  1.10s/it]\n"]},{"name":"stdout","output_type":"stream","text":["epoch no: 1 ||Train_loss : 0.0007003115606494248\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2:  95%|█████████▌| 2383/2500 [43:50<02:11,  1.13s/it]"]}],"source":["train(model,optimizer,device,num_train_epochs)"]},{"cell_type":"markdown","source":["**EVALUATION LOOP**"],"metadata":{"id":"2IK4jlODTmzM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLBMWO7Yob06"},"outputs":[],"source":["def train(model,optimizer,device,epochs):\n","  model.eval()\n","  val_loss = 0\n","  with torch.no_grad():\n","    with tqdm(total=len(eval_dataloader), desc='Validation Epoch {}'.format(epoch + 1)) as pbar:\n","      for batch in eval_dataloader:\n","          data,gif_embeds = batch\n","          data = {k:v.to(device) for k,v in data.items()}\n","          optimizer.zero_grad()\n","          final_output = model(input_ids = data['input_ids'].squeeze(dim=1), attention_mask = data['attention_mask'].squeeze(dim=1) , video_pixel_values = gif_embeds )\n","          logits = final_output['logits'].to(device)\n","          trimmed_logits = logits[:, 16:47, :].contiguous()\n","          trimmed_labels = data['input_ids'].squeeze(dim=1)[:, 1:].contiguous()\n","          loss = F.cross_entropy(trimmed_logits.view(-1, trimmed_logits.size(-1)), trimmed_labels.view(-1))\n","          val_loss+= loss.item()\n","          pbar.update(1)\n","\n","  avg_val_loss = val_loss/len(eval_dataloader)\n","  print(f'epoch no: {epoch + 1} ||eval_loss : {avg_val_loss}')   #batch avg loss in every epoch\n",""]},{"cell_type":"markdown","source":["**GENERATION LOOP**"],"metadata":{"id":"p2u5Rg18T3Dx"}},{"cell_type":"code","source":["with torch.no_grad():\n","  for batch in test_dataloader:\n","    data,gif_embeds,question,answer = batch\n","    input_ids = data['input_ids'].to(device)\n","    mask = data['attention_mask'].to(device)\n","    llama_embeddings , mask = model.generate(gif_embeds,input_ids,mask)\n","    output = model.peft_model.generate(\n","                            inputs_embeds=llama_embeddings.half(),\n","                            attention_mask=mask.half(),\n","                            max_new_tokens=16,\n","                            num_beams=5,\n","                            early_stopping=True,\n","                            num_return_sequences=1,\n","                            no_repeat_ngram_size=2\n","                        )\n","    for i in range(output.shape[0]):\n","      generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n","      print(f\"question : {question[i]}\")\n","      print(f\"predicted_answer : {generated_text}\")\n","      print(f\"actual_answer : {answer[i]}\")"],"metadata":{"id":"HQ3KpTXvTkYQ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"mount_file_id":"1dC7TrvzGJrQffybB0bIy06CZWwkag3Jj","authorship_tag":"ABX9TyPBXX6vRaEuS+YrfPUk/IVj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"496e851106d5423588a946c22327fd83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98c092af2c6d43eea4c14fd2b250fb7c","IPY_MODEL_e64464dde4ec494e9db3d9a72010cf46","IPY_MODEL_740f4fb9448a4c2a9c22fd8c81442a5b"],"layout":"IPY_MODEL_7a787f25ff424ac5b33fade4baf2184d"}},"506ede5c124b46a39fa79031c52dd630":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"740f4fb9448a4c2a9c22fd8c81442a5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e420c295c2f04e34bcb790489aad1b59","placeholder":"​","style":"IPY_MODEL_f73f9ea555194ab58f4046f766c33c74","value":" 2/2 [00:04&lt;00:00,  2.02s/it]"}},"7a787f25ff424ac5b33fade4baf2184d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98c092af2c6d43eea4c14fd2b250fb7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0f692048fd24ec093967104cc119030","placeholder":"​","style":"IPY_MODEL_c7441276316e48b2bdcef12cfc0e06c1","value":"Loading checkpoint shards: 100%"}},"a0f692048fd24ec093967104cc119030":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4c4e9d45c9b49938ac2f3625b3a72a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7441276316e48b2bdcef12cfc0e06c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e420c295c2f04e34bcb790489aad1b59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e64464dde4ec494e9db3d9a72010cf46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_506ede5c124b46a39fa79031c52dd630","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4c4e9d45c9b49938ac2f3625b3a72a5","value":2}},"f73f9ea555194ab58f4046f766c33c74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}